{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPk2a3kNlFFCvAMrilTlU/l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aradeyal/machine_learning/blob/main/callback_%D7%AA%D7%A8%D7%92%D7%99%D7%9C_%D7%AA%D7%99%D7%90%D7%95%D7%A8%D7%98%D7%99%E2%80%8E.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "\n",
        "# ========================= Callbacks =========================\n",
        "class EarlyStoppingAtMinLoss(keras.callbacks.Callback):\n",
        "    \"\"\"Stop when train loss plateaus OR when overfitting is suspected.\"\"\"\n",
        "    def __init__(self, patience=0):\n",
        "        super().__init__()\n",
        "        self.patience = int(patience)\n",
        "\n",
        "        # Overfitting logic (can be tuned after init)\n",
        "        self.overfit_patience = 2     # consecutive overfit-epochs allowed\n",
        "        self.min_abs_delta = 0.05     # trigger if (val_loss - loss) >= this\n",
        "        self.min_rel_delta = 0.20     # OR if (val_loss - loss)/loss >= this\n",
        "        self.warmup = 5               # skip overfit checks for first N epochs\n",
        "        self.restore_best_weights = True\n",
        "        self.verbose = 1\n",
        "\n",
        "        # Trackers (train loss)\n",
        "        self.best = np.inf\n",
        "        self.best_weights = None\n",
        "        self.wait = 0\n",
        "\n",
        "        # Trackers (overfitting)\n",
        "        self.best_val = np.inf\n",
        "        self.best_val_weights = None\n",
        "        self.wait_overfit = 0\n",
        "        self.stopped_epoch = 0\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        self.wait = 0\n",
        "        self.best = np.inf\n",
        "        self.best_weights = None\n",
        "        self.best_val = np.inf\n",
        "        self.best_val_weights = None\n",
        "        self.wait_overfit = 0\n",
        "        self.stopped_epoch = 0\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        loss = logs.get(\"loss\")\n",
        "        val_loss = logs.get(\"val_loss\")\n",
        "\n",
        "        # ---- original: stop if train loss stops improving ----\n",
        "        if loss is not None:\n",
        "            if loss < self.best:\n",
        "                self.best = loss\n",
        "                self.wait = 0\n",
        "                self.best_weights = self.model.get_weights()\n",
        "            else:\n",
        "                self.wait += 1\n",
        "                if self.verbose and self.patience:\n",
        "                    print(f\"[Train no-improve] wait {self.wait}/{self.patience}\")\n",
        "                if self.patience and self.wait >= self.patience:\n",
        "                    self.stopped_epoch = epoch + 1\n",
        "                    if self.verbose:\n",
        "                        print(\"Stopping (train loss plateau). Restoring best train-loss weights.\")\n",
        "                    self.model.stop_training = True\n",
        "                    if self.best_weights is not None:\n",
        "                        self.model.set_weights(self.best_weights)\n",
        "                    return  # already stopping\n",
        "\n",
        "        # ---- add-on: suspected overfitting stop ----\n",
        "        if loss is not None and val_loss is not None and (epoch + 1) > self.warmup:\n",
        "            # keep checkpoint by best val_loss\n",
        "            if val_loss < self.best_val:\n",
        "                self.best_val = val_loss\n",
        "                if self.restore_best_weights:\n",
        "                    self.best_val_weights = self.model.get_weights()\n",
        "\n",
        "            gap_abs = val_loss - loss\n",
        "            gap_rel = gap_abs / max(loss, 1e-8)\n",
        "            overfit_now = (gap_abs >= self.min_abs_delta) or (gap_rel >= self.min_rel_delta)\n",
        "\n",
        "            if overfit_now:\n",
        "                self.wait_overfit += 1\n",
        "                if self.verbose:\n",
        "                    print(f\"[Overfit?] epoch {epoch+1}: loss={loss:.4f}, val_loss={val_loss:.4f}, \"\n",
        "                          f\"gap_abs={gap_abs:.4f}, gap_rel={gap_rel:.2%} \"\n",
        "                          f\"(wait {self.wait_overfit}/{self.overfit_patience})\")\n",
        "                if self.wait_overfit >= self.overfit_patience:\n",
        "                    self.stopped_epoch = epoch + 1\n",
        "                    if self.verbose:\n",
        "                        print(\"Overfitting detected — stopping and restoring best val_loss weights.\")\n",
        "                    self.model.stop_training = True\n",
        "                    if self.restore_best_weights and self.best_val_weights is not None:\n",
        "                        self.model.set_weights(self.best_val_weights)\n",
        "            else:\n",
        "                self.wait_overfit = 0\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        if self.stopped_epoch > 0 and self.verbose:\n",
        "            print(f\"Epoch {self.stopped_epoch}: early stopping\")\n",
        "\n",
        "class LossAndErrorPrintingCallback(keras.callbacks.Callback):\n",
        "    \"\"\"Print loss & val_loss every epoch.\"\"\"\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        loss = logs.get(\"loss\")\n",
        "        val_loss = logs.get(\"val_loss\")\n",
        "        loss_str = f\"{loss:.6f}\" if loss is not None else \"NA\"\n",
        "        val_str  = f\"{val_loss:.6f}\" if val_loss is not None else \"NA\"\n",
        "        print(f\"Epoch {epoch+1} — loss: {loss_str}, val_loss: {val_str}\")\n",
        "\n",
        "# ========================= Model =========================\n",
        "def get_model(n_features: int):\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Input(shape=(n_features,)),\n",
        "        keras.layers.Dense(64, activation=\"relu\"),\n",
        "        keras.layers.Dense(1)  # regression head (for classification use softmax)\n",
        "    ])\n",
        "    model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "    return model\n",
        "\n",
        "# ========================= TRAIN =========================\n",
        "# Expect x_train, y_train to be defined by you:\n",
        "# x_train: shape (num_samples, num_features)\n",
        "# y_train: shape (num_samples,)\n",
        "# If you don't have them yet, load/prepare your data above.\n",
        "\n",
        "# n_features from your data\n",
        "n_features = x_train.shape[1]\n",
        "\n",
        "model = get_model(n_features)\n",
        "\n",
        "# configure the callback (so training can go further before stopping for overfitting)\n",
        "cb = EarlyStoppingAtMinLoss(patience=5)\n",
        "cb.min_rel_delta = 0.35     # less sensitive relative gap (was 0.20)\n",
        "cb.min_abs_delta = 0.07     # less sensitive absolute gap (was 0.05)\n",
        "cb.overfit_patience = 4     # require more consecutive overfit epochs (was 2)\n",
        "cb.warmup = 10              # start checking later (was 5)\n",
        "\n",
        "# (optional) lower LR when val_loss plateaus — helps loss go down further\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-6, verbose=1\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    validation_split=0.2,      # or validation_data=(x_val, y_val)\n",
        "    epochs=60,\n",
        "    batch_size=64,\n",
        "    verbose=0,                 # logs printed by callbacks\n",
        "    callbacks=[LossAndErrorPrintingCallback(), cb, reduce_lr],\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AwOsxAgvOXn",
        "outputId": "af50b6fb-0eb7-4c2e-9c91-d502f604e56a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 — loss: 7.702114, val_loss: 7.758357\n",
            "Epoch 2 — loss: 6.601505, val_loss: 6.592540\n",
            "Epoch 3 — loss: 5.558546, val_loss: 5.516394\n",
            "Epoch 4 — loss: 4.583126, val_loss: 4.438117\n",
            "Epoch 5 — loss: 3.607121, val_loss: 3.433881\n",
            "Epoch 6 — loss: 2.708570, val_loss: 2.540847\n",
            "Epoch 7 — loss: 1.949201, val_loss: 1.796958\n",
            "Epoch 8 — loss: 1.318943, val_loss: 1.239800\n",
            "Epoch 9 — loss: 0.901448, val_loss: 0.853298\n",
            "Epoch 10 — loss: 0.639529, val_loss: 0.617327\n",
            "Epoch 11 — loss: 0.483727, val_loss: 0.490222\n",
            "Epoch 12 — loss: 0.403558, val_loss: 0.422356\n",
            "Epoch 13 — loss: 0.365822, val_loss: 0.382997\n",
            "Epoch 14 — loss: 0.343832, val_loss: 0.363323\n",
            "Epoch 15 — loss: 0.330520, val_loss: 0.351166\n",
            "Epoch 16 — loss: 0.321747, val_loss: 0.342800\n",
            "Epoch 17 — loss: 0.316055, val_loss: 0.336048\n",
            "Epoch 18 — loss: 0.309998, val_loss: 0.332338\n",
            "Epoch 19 — loss: 0.304692, val_loss: 0.327520\n",
            "Epoch 20 — loss: 0.300473, val_loss: 0.324472\n",
            "Epoch 21 — loss: 0.296417, val_loss: 0.320107\n",
            "Epoch 22 — loss: 0.292141, val_loss: 0.317480\n",
            "Epoch 23 — loss: 0.288899, val_loss: 0.314212\n",
            "Epoch 24 — loss: 0.285739, val_loss: 0.310954\n",
            "Epoch 25 — loss: 0.282832, val_loss: 0.309457\n",
            "Epoch 26 — loss: 0.280797, val_loss: 0.308359\n",
            "Epoch 27 — loss: 0.277227, val_loss: 0.306917\n",
            "Epoch 28 — loss: 0.275182, val_loss: 0.305056\n",
            "Epoch 29 — loss: 0.272973, val_loss: 0.302897\n",
            "Epoch 30 — loss: 0.270972, val_loss: 0.302767\n",
            "Epoch 31 — loss: 0.269712, val_loss: 0.302294\n",
            "Epoch 32 — loss: 0.267690, val_loss: 0.301137\n",
            "Epoch 33 — loss: 0.265493, val_loss: 0.299573\n",
            "Epoch 34 — loss: 0.264184, val_loss: 0.297940\n",
            "Epoch 35 — loss: 0.262276, val_loss: 0.297852\n",
            "Epoch 36 — loss: 0.260972, val_loss: 0.297934\n",
            "Epoch 37 — loss: 0.260404, val_loss: 0.297659\n",
            "Epoch 38 — loss: 0.258795, val_loss: 0.295291\n",
            "Epoch 39 — loss: 0.257233, val_loss: 0.297268\n",
            "Epoch 40 — loss: 0.255836, val_loss: 0.296782\n",
            "Epoch 41 — loss: 0.255028, val_loss: 0.297092\n",
            "\n",
            "Epoch 41: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "Epoch 42 — loss: 0.253227, val_loss: 0.295614\n",
            "Epoch 43 — loss: 0.252741, val_loss: 0.296074\n",
            "Epoch 44 — loss: 0.252166, val_loss: 0.295887\n",
            "\n",
            "Epoch 44: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "Epoch 45 — loss: 0.251464, val_loss: 0.295662\n",
            "Epoch 46 — loss: 0.251292, val_loss: 0.296393\n",
            "Epoch 47 — loss: 0.250857, val_loss: 0.296117\n",
            "\n",
            "Epoch 47: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "Epoch 48 — loss: 0.250513, val_loss: 0.296043\n",
            "Epoch 49 — loss: 0.250344, val_loss: 0.296089\n",
            "Epoch 50 — loss: 0.250231, val_loss: 0.295835\n",
            "\n",
            "Epoch 50: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "Epoch 51 — loss: 0.250070, val_loss: 0.295790\n",
            "Epoch 52 — loss: 0.249971, val_loss: 0.295714\n",
            "Epoch 53 — loss: 0.249875, val_loss: 0.295754\n",
            "\n",
            "Epoch 53: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "Epoch 54 — loss: 0.249812, val_loss: 0.295679\n",
            "Epoch 55 — loss: 0.249790, val_loss: 0.295704\n",
            "Epoch 56 — loss: 0.249739, val_loss: 0.295752\n",
            "\n",
            "Epoch 56: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "Epoch 57 — loss: 0.249701, val_loss: 0.295709\n",
            "Epoch 58 — loss: 0.249662, val_loss: 0.295703\n",
            "Epoch 59 — loss: 0.249651, val_loss: 0.295695\n",
            "\n",
            "Epoch 59: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "Epoch 60 — loss: 0.249620, val_loss: 0.295700\n"
          ]
        }
      ]
    }
  ]
}